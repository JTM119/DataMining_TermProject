{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PlantDiseaseClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GTceK89L1NbK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/Drive', force_remount=True)\n",
        "import os\n",
        "os.chdir('/content/Drive/My Drive/DataMining_Project')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oki1ab6u1c_O",
        "outputId": "91bca07a-26bf-4698-e40f-854e2aad9656"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "Rbl7zB0Z2n_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downblock"
      ],
      "metadata": {
        "id": "JLUIxASK2rBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  A downsample block consists of a \n",
        "    - Convolutional Layer\n",
        "    - Batch Norm Layer\n",
        "    - Activation Function (This can be experimented with as necessary)\n",
        "\n",
        "\"\"\"\n",
        "def downsample(stride = 2, filters = 32, dropout = 0):\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(tf.keras.layers.Conv2D(kernel_size = 2, strides = stride,  filters = filters,  kernel_regularizer='l2'))\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "  result.add(tf.keras.layers.Dropout(dropout))\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "  return result"
      ],
      "metadata": {
        "id": "ZaHwYntJ2tvY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architectures"
      ],
      "metadata": {
        "id": "pYHpgyYe2vmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shallow"
      ],
      "metadata": {
        "id": "m-jrlRJ13DVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shallow_CNN(model_type = None, dropout = 0,  in_shape = (256,256,3)):\n",
        "  assert model_type in [\"disease\", \"plant\"]\n",
        "  current_image = 256\n",
        "  downsample_blocks = []\n",
        "  #Determine how many cnn blocks are necessary\n",
        "  \n",
        "\n",
        "  #Create the input\n",
        "  input = tf.keras.Input(shape =  in_shape )\n",
        "  next = input\n",
        "  next = tf.keras.layers.Conv2D(kernel_size = 256, strides = 1,  filters = 256,  kernel_regularizer='l2')(next)\n",
        "  next = tf.keras.layers.BatchNormalization()(next)\n",
        "  next = tf.keras.layers.ReLU()(next)\n",
        "  #Get a fully connected layer : This will be used to extract the input for the random forest and the SVM models\n",
        "  next = tf.keras.layers.Flatten()(next)\n",
        "  next = tf.keras.layers.Dense(100,  kernel_regularizer='l2')(next)\n",
        "  \n",
        "  #Add another dense layer to resize to number of classes\n",
        "  if model_type == \"plant\":\n",
        "    next = tf.keras.layers.Dense(11)(next)\n",
        "  else:\n",
        "    next = tf.keras.layers.Dense(38)(next)\n",
        "  \n",
        "  #Turn the final layer into a probability layer to get the predictions\n",
        "  final = tf.keras.layers.Softmax()(next)  \n",
        "  return tf.keras.Model(inputs = input, outputs = final)"
      ],
      "metadata": {
        "id": "3VJQUFBG2-YO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic"
      ],
      "metadata": {
        "id": "kBxCXn8q2zIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_CNN(model_type = None, dropout = 0,  in_shape = (256,256,3)):\n",
        "  assert model_type in [\"disease\", \"plant\"]\n",
        "  print(in_shape)\n",
        "  current_image = 256\n",
        "  downsample_blocks = []\n",
        "  #Determine how many cnn blocks are necessary\n",
        "  while current_image > 1:\n",
        "    \n",
        "    downsample_blocks.append(downsample(stride = 2, filters = 128, dropout = dropout))\n",
        "    current_image = current_image/2\n",
        "\n",
        "  #Create the input\n",
        "  input = tf.keras.Input(shape =  in_shape )\n",
        "  next = input\n",
        "  #Pass it through each CNN layer\n",
        "  for downblock in downsample_blocks:\n",
        "    next = downblock(next)\n",
        "    \n",
        "  #Get a fully connected layer : This will be used to extract the input for the random forest and the SVM models\n",
        "  next = tf.keras.layers.Flatten()(next)\n",
        "  next = tf.keras.layers.Dense(100,  kernel_regularizer='l2')(next)\n",
        "  \n",
        "  #Add another dense layer to resize to number of classes\n",
        "  if model_type == \"plant\":\n",
        "    next = tf.keras.layers.Dense(11)(next)\n",
        "  else:\n",
        "    next = tf.keras.layers.Dense(38)(next)\n",
        "  \n",
        "  #Turn the final layer into a probability layer to get the predictions\n",
        "  final = tf.keras.layers.Softmax()(next)  \n",
        "  return tf.keras.Model(inputs = input, outputs = final)\n"
      ],
      "metadata": {
        "id": "1Fh3HAY-2ykL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small"
      ],
      "metadata": {
        "id": "lm4V8OnW3XpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def small_CNN(model_type = None, dropout = 0,  in_shape = (256,256,3)):\n",
        "  assert model_type in [\"disease\", \"plant\"]\n",
        "  current_image = 256\n",
        "  downsample_blocks = []\n",
        "  #Determine how many cnn blocks are necessary\n",
        "  downsample_blocks = [\n",
        "                       downsample(stride = 2, filters = 8, dropout = dropout),   #output shape : [None, 128,128, 8]\n",
        "                       downsample(stride = 2, filters = 16, dropout = dropout),   #output shape : [None, 64, 64, 16]\n",
        "                       downsample(stride = 2, filters = 32, dropout = dropout),   #output shape : [None, 32,32, 32]\n",
        "                       downsample(stride = 2, filters = 64, dropout = dropout),   #output shape : [None, 16,16, 64]\n",
        "                       downsample(stride = 2, filters = 128, dropout = dropout),   #output shape : [None, 8,8, 128]\n",
        "                       downsample(stride = 2, filters = 128, dropout = dropout),   #output shape : [None, 4,4, 128]\n",
        "                       downsample(stride = 2, filters = 128, dropout = dropout),   #output shape : [None, 2,2, 128]\n",
        "                       downsample(stride = 2, filters = 128, dropout = dropout),   #output shape : [None, 1,1, 128]\n",
        "                      ]\n",
        "\n",
        "  #Create the input\n",
        "  input = tf.keras.Input(shape =  in_shape )\n",
        "  next = input\n",
        "  #Pass it through each CNN layer\n",
        "  for downblock in downsample_blocks:\n",
        "    next = downblock(next)\n",
        "\n",
        "  #Get a fully connected layer : This will be used to extract the input for the random forest and the SVM models\n",
        "  next = tf.keras.layers.Flatten()(next)\n",
        "  next = tf.keras.layers.Dense(100,  kernel_regularizer='l2')(next)\n",
        "  \n",
        "  if model_type == \"plant\":\n",
        "    next = tf.keras.layers.Dense(11)(next)\n",
        "  else:\n",
        "    next = tf.keras.layers.Dense(38)(next)\n",
        "  \n",
        "  final = tf.keras.layers.Softmax()(next)  \n",
        "  \n",
        "  return tf.keras.Model(inputs = input, outputs = final)"
      ],
      "metadata": {
        "id": "Xt0pdJt83ZkB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tiny"
      ],
      "metadata": {
        "id": "wHt0b8253fwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tiny_CNN(model_type = None, dropout = 0,  in_shape = (256,256,3)):\n",
        "  current_image = 256\n",
        "  downsample_blocks = []\n",
        "  #Determine how many cnn blocks are necessary\n",
        "  downsample_blocks = [\n",
        "                       downsample(stride = 4, filters = 4, dropout = dropout),    #output shape : [None, 64, 64, 4]\n",
        "                       downsample(stride = 4, filters = 8, dropout = dropout),   #output shape : [None, 16, 16, 8]\n",
        "                       downsample(stride = 4, filters = 16, dropout = dropout),   #output shape : [None, 4, 4, 16]\n",
        "                       downsample(stride = 4, filters = 32, dropout = dropout),   #output shape : [None, 1, 1, 32]\n",
        "                      ]\n",
        "\n",
        "  #Create the input\n",
        "  input = tf.keras.Input(shape = in_shape)\n",
        "  next = input\n",
        "  #Pass it through each CNN layer\n",
        "  for downblock in downsample_blocks:\n",
        "    next = downblock(next)\n",
        "    #print(next.shape)\n",
        "\n",
        "  #Get a fully connected layer : This will be used to extract the input for the random forest and the SVM models\n",
        "  next = tf.keras.layers.Flatten()(next)\n",
        "  next = tf.keras.layers.Dense(100,  kernel_regularizer='l2')(next)\n",
        "  #print(next.shape)\n",
        "  #Add another dense layer to resize to number of classes\n",
        "  if model_type == \"plant\":\n",
        "    next = tf.keras.layers.Dense(11)(next)\n",
        "  else:\n",
        "    next = tf.keras.layers.Dense(38)(next)\n",
        "  #print(next.shape)\n",
        "  #Turn the final layer into a probability layer to get the predictions\n",
        "  final = tf.keras.layers.Softmax()(next)  \n",
        "  #final = tf.math.argmax(final, axis = -1).int64()  \n",
        "  #print(type(final))\n",
        "  return tf.keras.Model(inputs = input, outputs = final)"
      ],
      "metadata": {
        "id": "AO4YZu1x3hjq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset"
      ],
      "metadata": {
        "id": "ameUaCYW3nY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(path, batch_size):\n",
        "  dataset_location = path\n",
        "  if \"processed\" in path:\n",
        "    train_data = tf.keras.utils.image_dataset_from_directory(dataset_location, \n",
        "                                                            labels = \"inferred\", \n",
        "                                                            label_mode = \"categorical\", \n",
        "                                                            image_size = (256,256),\n",
        "                                                            subset =  \"training\",\n",
        "                                                            validation_split = .2,\n",
        "                                                            color_mode = \"rgba\",\n",
        "                                                            batch_size = batch_size,\n",
        "                                                            shuffle = True,\n",
        "                                                            seed = 1)\n",
        "    validation_data = tf.keras.utils.image_dataset_from_directory(dataset_location, \n",
        "                                                              labels = \"inferred\", \n",
        "                                                              label_mode = \"categorical\", \n",
        "                                                              image_size = (256,256),\n",
        "                                                              subset =  \"validation\",\n",
        "                                                              validation_split = .2,\n",
        "                                                              batch_size = batch_size,\n",
        "                                                              color_mode = \"rgba\",\n",
        "                                                              shuffle = True,\n",
        "                                                              seed = 1)\n",
        "    return train_data, validation_data\n",
        "  train_data = tf.keras.utils.image_dataset_from_directory(dataset_location, \n",
        "                                                            labels = \"inferred\", \n",
        "                                                            label_mode = \"categorical\", \n",
        "                                                            image_size = (256,256),\n",
        "                                                            subset =  \"training\",\n",
        "                                                            validation_split = .2,\n",
        "                                                            batch_size = batch_size,\n",
        "                                                            shuffle = True,\n",
        "                                                            seed = 1)\n",
        "  validation_data = tf.keras.utils.image_dataset_from_directory(dataset_location, \n",
        "                                                            labels = \"inferred\", \n",
        "                                                            label_mode = \"categorical\", \n",
        "                                                            image_size = (256,256),\n",
        "                                                            subset =  \"validation\",\n",
        "                                                            validation_split = .2,\n",
        "                                                            batch_size = batch_size,\n",
        "                                                            shuffle = True,\n",
        "                                                            seed = 1)\n",
        "  return train_data, validation_data"
      ],
      "metadata": {
        "id": "Mz220SL73ubW"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_class_weights(path):\n",
        "  counts = []    \n",
        "  old_location = path\n",
        "  for i,dir in enumerate(os.walk(old_location)):\n",
        "      \n",
        "      if dir[0] == path:\n",
        "        continue\n",
        "      counts.append(len(os.listdir(dir[0])))\n",
        "  max_count = max(counts)\n",
        "  class_weights = max_count / np.array(counts )\n",
        "  weights = {}\n",
        "  for i, weight in enumerate(class_weights):\n",
        "    weights[i] = weight\n",
        "  return weights \n"
      ],
      "metadata": {
        "id": "zvNuZ7Xu3xrw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "hCSIeGVw32Rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_path, log_dir, ckpt_dir, batch_size):\n",
        "  train_data, validation_data = create_dataset(data_path, batch_size = batch_size)\n",
        "  class_weights = create_class_weights(data_path)\n",
        "  tf_callbacks = tf.keras.callbacks.TensorBoard(log_dir = log_dir, update_freq = 1 )\n",
        "  tf_modelCkpt = tf.keras.callbacks.ModelCheckpoint(ckpt_dir, monitor=\"val_loss\", save_best_only=True,  save_weights_only=False, mode=\"auto\", save_freq='epoch', initial_value_threshold=None)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss = tf.keras.losses.CategoricalCrossentropy(), metrics = [\"accuracy\", \"mae\"])\n",
        "  model.fit(train_data, epochs = 150, validation_freq = 2, validation_data = validation_data, class_weight = class_weights, callbacks = [tf_callbacks, tf_modelCkpt])"
      ],
      "metadata": {
        "id": "Jly7Ez_M41dV"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Shallow"
      ],
      "metadata": {
        "id": "BJuatEZL4i4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(shallow_CNN(model_type = \"disease\", dropout = 0, in_shape = (256,256,4)), \"data/processed\",'saved_models/shallow/logs', \"saved_models/shallow/checkpoints/\", 32 )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "LSCOxqd735bx",
        "outputId": "33522b2e-ea89-41f4-ee7e-4869cde83187"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-bda121a23267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshallow_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"disease\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/processed\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'saved_models/shallow/logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"saved_models/shallow/checkpoints/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-4de44bc9792b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_path, log_dir, ckpt_dir, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_class_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtf_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtf_modelCkpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_value_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-3f44d21c4cef>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(path, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                             \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                                             seed = 1)\n\u001b[0m\u001b[1;32m     14\u001b[0m     validation_data = tf.keras.utils.image_dataset_from_directory(dataset_location, \n\u001b[1;32m     15\u001b[0m                                                               \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m       follow_links=follow_links)\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0mlabels_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mpartial_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mlabels_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpartial_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coarse Search for Optimal"
      ],
      "metadata": {
        "id": "h6LBNYne6SB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [basic_CNN, small_CNN, tiny_CNN]\n",
        "model_name = [\"basic\", \"small\", \"tiny\"]\n",
        "dropouts = [0,.05,.1,.15]\n",
        "for m, mn in zip(models, model_name):\n",
        "  for d in dropouts:\n",
        "    train_model(m(model_type = \"disease\", dropout = d, in_shape = (256,256,4)), \"data/processed\" ,f'saved_models/processed/{mn}/{d}/logs', f\"saved_models/processed/{mn}/{d*100}/checkpoints/\", 32 )\n",
        "    train_model(m(model_type = \"disease\", dropout = d, in_shape = (256,256,3)), \"data/raw\" ,f'saved_models/raw/{mn}/{d}/logs', f\"saved_models/raw/{mn}/{d*100}/checkpoints/\", 32 )"
      ],
      "metadata": {
        "id": "O2ymBCvN6Mpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning"
      ],
      "metadata": {
        "id": "GhDttCan7XbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet101_model(in_shape = (256,256,3)):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  input = tf.keras.Input(shape = in_shape)\n",
        "  resnet = tf.keras.applications.ResNet101V2(input_tensor = input)\n",
        "  model.add(resnet)\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(100))\n",
        "  model.add(tf.keras.layers.Dense(38, activation = \"softmax\"))\n",
        "  return model\n",
        "\n",
        "def resnet50_model(in_shape = (256,256,3)):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  input = tf.keras.Input(shape = in_shape)\n",
        "  resnet = tf.keras.applications.ResNet50(input_tensor = input)\n",
        "  model.add(resnet)\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(100))\n",
        "  model.add(tf.keras.layers.Dense(38, activation = \"softmax\"))\n",
        "  return model\n",
        "  \n",
        "def mobilenet_model(in_shape = (256,256,3)):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  input = tf.keras.Input(shape = in_shape)\n",
        "  resnet = tf.keras.applications.MobileNetV2(input_tensor = input)\n",
        "  model.add(resnet)\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(100))\n",
        "  model.add(tf.keras.layers.Dense(38, activation = \"softmax\"))\n",
        "  return model\n",
        "\n",
        "def inception_model(in_shape = (256,256,3)):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  input = tf.keras.Input(shape = in_shape)\n",
        "  resnet = tf.keras.applications.InceptionV3(input_tensor = input)\n",
        "  model.add(resnet)\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(100))\n",
        "  model.add(tf.keras.layers.Dense(38, activation = \"softmax\"))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "47WYVOLf7Wez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(resnet50_model(), \"data/raw\" ,f'saved_models/ResNet50/logs', f\"saved_models/ResNet50/checkpoints/\", 32 )"
      ],
      "metadata": {
        "id": "PCPNsAV29t91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_model(resnet101_model, \"data/raw\" ,f'saved_models/ResNet101/logs', f\"saved_models/ResNet101/checkpoints/\", 32 )"
      ],
      "metadata": {
        "id": "NTrXRE10-cAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_model(inception_model, \"data/raw\" ,f'saved_models/InceptionV3/logs', f\"saved_models/InceptionV3/checkpoints/\", 32 )"
      ],
      "metadata": {
        "id": "8Zwu8e40-cua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_model(mobilenet_model, \"data/raw\" ,f'saved_models/MobileNet/logs', f\"saved_models/MobileNet/checkpoints/\", 32 )"
      ],
      "metadata": {
        "id": "b9Akgbmr-j-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM and RF with Best Model"
      ],
      "metadata": {
        "id": "bAYxoQlf3-gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('saved_models/ResNet101V2_continued/checkpoints')"
      ],
      "metadata": {
        "id": "HLvl_oxp394v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease_list = []\n",
        "for i,dir in enumerate(os.walk(\"data/raw\")):\n",
        "  if dir[0] == \"data/raw\":\n",
        "    continue\n",
        "  disease_list.append(dir[0][dir[0].rfind('/')+1:])\n",
        "disease_list.sort()\n",
        "labels_dict = {}\n",
        "for x in range(len(disease_list)):\n",
        "  labels_dict[x] = disease_list[x]\n"
      ],
      "metadata": {
        "id": "VwllVi3n4UMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'data/raw'\n",
        "train_data, validation_data = create_dataset(path, batch_size = 32)"
      ],
      "metadata": {
        "id": "cy2KVGu14Wtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = create_class_weights(\"data/raw\")\n",
        "print(class_weights)\n",
        "rf_weights = {}\n",
        "for x, y in class_weights.items():\n",
        "  rf_weights[labels_dict[x]] = y\n",
        "rf_weights"
      ],
      "metadata": {
        "id": "OYehiKCA4eFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_x = []\n",
        "val_y = []\n",
        "\n",
        "intermediate_layer_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "\n",
        "\n",
        "for x in validation_data:\n",
        "  for row in x[1]:\n",
        "    val_y.append(labels_dict[int(tf.math.argmax(row))])\n",
        "  temp = intermediate_layer_model.predict(x[0])\n",
        "  temp = np.array(temp)\n",
        "  for row in temp:\n",
        "    temp_row = row.reshape(100)\n",
        "    val_x.append(row)\n",
        "\n",
        "train_x = []\n",
        "train_y = []\n",
        "\n",
        "for x in train_data:\n",
        "  for row in x[1]:\n",
        "    train_y.append(labels_dict[int(tf.math.argmax(row))])\n",
        "  temp = intermediate_layer_model.predict(x[0])\n",
        "  temp = np.array(temp)\n",
        "  for row in temp:\n",
        "    temp_row = row.reshape(100)\n",
        "    train_x.append(row)\n",
        "  "
      ],
      "metadata": {
        "id": "amfFJk404a7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
        "# Instantiate model with 1000 decision trees\n",
        "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
        "# Train the model on training data\n",
        "#rf.fit(train_features, train_labels);\n",
        "rf.fit(train_x, train_y)\n"
      ],
      "metadata": {
        "id": "jS_LIVSZ6zsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "predictions = rf.predict(val_x)\n",
        "print(confusion_matrix(val_y, predictions))\n",
        "print(classification_report(val_y,predictions))"
      ],
      "metadata": {
        "id": "yBWhqtj37Q_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svclassifier = SVC(kernel='linear')\n",
        "svclassifier.fit(train_x, train_y)"
      ],
      "metadata": {
        "id": "TXp0ks2z4O5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = svclassifier.predict(val_x)\n",
        "print(confusion_matrix(val_y, predictions))\n",
        "print(classification_report(val_y,predictions))"
      ],
      "metadata": {
        "id": "KSnK-KVJA6Y_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}